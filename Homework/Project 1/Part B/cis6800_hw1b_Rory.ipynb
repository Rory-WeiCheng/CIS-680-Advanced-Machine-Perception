{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_PGiTnKFFTNi"
      },
      "source": [
        "# CIS6800: Project 1b: Deep Learning Basics Part B\n",
        "\n",
        "### Instructions:\n",
        "* This is an individual assignment. Collaborating with others is not permitted.\n",
        "* There is no single answer to most problems in deep learning, therefore the questions will often be underspecified. You need to fill in the blanks and submit a solution that solves the (practical) problem. Document the choices (hyperparameters, features, neural network architectures, etc.) you made where specified.\n",
        "* All the code should be written in Python. You should only use PyTorch to complete this project.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "Q41jvpsZFTNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444766b6-7731-4576-85e6-f8f431b1cf84"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "rng_seed = 1144\n",
        "\n",
        "# Download MNIST\n",
        "torchvision.datasets.MNIST('.', download=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: .\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BxU8GMDlFTNo"
      },
      "source": [
        "## 4. Adversarial Images (30%)\n",
        "In this part you will see how you can use the gradients of the network to generate adversarial\n",
        "images. Using these images that look almost identical the original you will be able to fool\n",
        "different neural networks. You will also see that these images also affect different neural\n",
        "networks and expose a security issue of CNNs that malicious users can take advantage of.\n",
        "An example is shown in Figure 4. You are encouraged to read the relevant papers [1, 2]\n",
        "before solving this part.\n",
        "\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW1/images/fig4.png\"/></div>\n",
        "\n",
        "<center>Figure 4: An adversarial example demonstrated in [1].</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol1GPUaqFTNp"
      },
      "source": [
        "1. (10%) Use the trained network from question 3 to generate adversarial images with\n",
        "constraints. The constraints that you have are:\n",
        "\n",
        "  1. You are not allowed to erase parts of the image, i.e. $I_\\text{pert} \\ge I$ at each pixel location.\n",
        "  2. The perturbed image has to take valid values, i.e. $-1 \\le I_\\text{pert} \\le 1$.\n",
        "\n",
        "  The algorithm works as follows:\n",
        "  \n",
        "  1. Let $I$ be a test image of your dataset that you want to perturb that is classified correctly by the network. Let $I_\\epsilon$ be the perturbation that you should initialize\n",
        "with zeros.\n",
        "  2. Feed $I_\\text{pert} = I + I_\\epsilon$ in the network.\n",
        "  3. Calculate the loss given the ground truth label ($y_\\mathrm{gt}$). Let the loss be $L(x,y |\\theta)$ where $\\theta$ are the learned weights.\n",
        "  4. Compute the gradients with respect to $I_\\text{pert}$, i.e., $\\nabla_{I_\\text{pert}} L(I_\\text{pert}, y_\\mathrm{gt} | \\theta)$. Using backpropagation, compute $\\nabla_{I_\\epsilon} L(I_\\epsilon,y_\\mathrm{gt} | \\theta)$, i.e. the gradients with respect to the perturbation.\n",
        "  5. Use the Fast Gradient Sign method to update the perturbation, i.e., $I_\\epsilon = I_\\epsilon + \\epsilon\\,\\text{sign}(\\nabla_{I_\\epsilon} L(I_\\epsilon, y_\\mathrm{gt}))$, where $\\epsilon$ is a small constant of your choice.\n",
        "  6. Repeat A-D until the network classify the input image $I_\\text{pert}$ as an arbitrary\n",
        "wrong category with confidence (probability) at least $90\\%$.\n",
        "\n",
        "  Generate 2 examples of adversarial images. Describe the difference between the adversarial images and the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "w4oRChtNFTNp"
      },
      "source": [
        "# Setups and Initializations for training\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# Create your network here (do not change this name)\n",
        "class DigitClassification(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(DigitClassification,self).__init__()\n",
        "      # Conv. and Pool. Layer 1\n",
        "      # Conv1 Input: 28*28*1, Kernel: 5*5*32 \n",
        "        # Padding 'same': means the output has the same size with input. Padding size: 5//2=2 (or floor(5/2))\n",
        "        # Output: 28*28*32, i.e. floor((W-K+2P)/S)+1 and floor((H-K+2P)/S)+1\n",
        "      # Pool1 Input: 28*28*32, Kernel: 2*2, Stride: 2, Output: 14*14*32\n",
        "      self.layer1 = nn.Sequential()\n",
        "      self.layer1.add_module(\"Conv1\", nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding='same'))\n",
        "      self.layer1.add_module(\"BN1\", nn.BatchNorm2d(num_features=32))\n",
        "      self.layer1.add_module(\"ReLu1\", nn.ReLU())\n",
        "      self.layer1.add_module(\"Pool1\",nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
        "\n",
        "      # Conv. and Pool Layer 2\n",
        "      # Conv2 Input: 14*14*32, Kernel: 5*5*32, Padding \"same\", Ouput: 14*14*32\n",
        "      # Pool2 Input: 14*14*32, Kernel: 2*2, Stride: 2, Output: 7*7*32\n",
        "      self.layer2 = nn.Sequential()\n",
        "      self.layer2.add_module(\"Conv2\", nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding='same'))\n",
        "      self.layer2.add_module(\"BN2\", nn.BatchNorm2d(num_features=32))\n",
        "      self.layer2.add_module(\"ReLu2\", nn.ReLU())\n",
        "      self.layer2.add_module(\"Pool2\",nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
        "\n",
        "      # Conv. and Pool Layer 3\n",
        "      # Conv3 Input: 7*7*32, Kernel: 5*5*64, Padding \"same\", Ouput: 7*7*64\n",
        "      # Pool3 Input: 7*7*64, Kernel: 2*2, Stride: 2, Output: 3*3*64\n",
        "      self.layer3 = nn.Sequential()\n",
        "      self.layer3.add_module(\"Conv3\", nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding='same'))\n",
        "      self.layer3.add_module(\"BN3\", nn.BatchNorm2d(num_features=64))\n",
        "      self.layer3.add_module(\"ReLu3\", nn.ReLU())\n",
        "      self.layer3.add_module(\"Pool3\",nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
        "\n",
        "      # Fully Connected 1\n",
        "      # Input: 3*3*64, Ouput: 64\n",
        "      self.fullycon1 = nn.Sequential()\n",
        "      self.fullycon1.add_module(\"FC1\", nn.Linear(in_features=3 * 3 * 64, out_features=64))\n",
        "      self.fullycon1.add_module(\"FC1BN\", nn.BatchNorm1d(num_features=64))\n",
        "      self.fullycon1.add_module(\"FC1ReLu\", nn.ReLU())\n",
        "\n",
        "      # Fully Connected 2\n",
        "      # Input: 64, Ouput: 10\n",
        "      self.fullycon2 = nn.Sequential()\n",
        "      self.fullycon2.add_module(\"FC2\", nn.Linear(in_features=64, out_features=10))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = x.view (-1, 3 * 3 * 64)\n",
        "    x = self.fullycon1(x)\n",
        "    x = self.fullycon2(x)\n",
        "    return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bpN5t009FTNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0be23ae0-f57f-494f-b5c9-7d058974415d"
      },
      "source": [
        "# don't change the signature of this function (image, image_pert -> [N, 1, H, W])\n",
        "def arbitrary_adversary(model, image, original_label):\n",
        "  # Setups and Initializations for training\n",
        "  # CUDA for PyTorch\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "  perturbation = torch.zeros(image.size()).to(device)\n",
        "  perturbation.requires_grad = True\n",
        "  max_step = 1000\n",
        "  eps = 0.02\n",
        "  for i in range(max_step):\n",
        "    # Start FGSM Attack, Follow the steps stated above\n",
        "    # perturbation : I_eps in HW document, image_per: I_pert in HW document\n",
        "    model.zero_grad()\n",
        "    # Contraints: take the valid values\n",
        "    output = model((image + perturbation).clamp(min=-1, max=1))\n",
        "    loss = F.cross_entropy(output, original_label)\n",
        "    loss.backward()\n",
        "    perturbation_grad = perturbation.grad.data\n",
        "    sign_perturbation_grad = perturbation_grad.sign()\n",
        "    # Contraints: perturbation should be positive (do not erase parts of the image)\n",
        "    # perturbation is a leaf tensor, so use .data to take the inplace operation\n",
        "    perturbation.data += eps * sign_perturbation_grad   \n",
        "    perturbation.data = perturbation.clamp(min=0)\n",
        "    # Contraints: take the valid values\n",
        "    image_pert = image + perturbation.data\n",
        "    image_pert = torch.clamp(image_pert, -1, 1)\n",
        "\n",
        "    # Predict and calculate the loss\n",
        "    output_pert = model(image_pert)\n",
        "    predicted = torch.max(output_pert.data, 1)[1]\n",
        "    confidence = torch.max(F.softmax(output_pert.data,dim=1))\n",
        "    if i % 10 == 1:\n",
        "      print('\\n Attack Iteration: {}, loss: {:.3f}, current_predict_label: {}\\n'.format(i-1,loss.detach().cpu().item(), \n",
        "      predicted.detach().cpu().item()))\n",
        "\n",
        "    if predicted.item() != original_label.item() and confidence >= 0.90:\n",
        "      print(\"\\n Attack Iteration: {}, misclassified label: {}, Confidence: {} \".format(i-1, predicted.detach().cpu().item(),confidence))\n",
        "      image_pert = image_pert.detach().cpu()\n",
        "      break\n",
        "  return image_pert\n",
        "\n",
        "# load two pictures and generate the adversarial images\n",
        "# Setps for image loaders. Normalize the input to (-1,1)\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "image_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform),\n",
        "    batch_size=1, shuffle=True)\n",
        "\n",
        "# Setups and Initializations for training\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# load the model\n",
        "model_path = 'model.pth'\n",
        "# Instantiate the network\n",
        "model = DigitClassification().to(device)\n",
        "# load the parameters saved\n",
        "model.eval().load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "cnt=0\n",
        "\n",
        "for i in range(100):\n",
        "  image_iterator=iter(image_loader)\n",
        "  image, label = image_iterator.next()\n",
        "  image, label = image.to(device), label.to(device)\n",
        "  output = model(image)\n",
        "  predicted = torch.max(output.data, 1)[1]\n",
        "  # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "  if predicted.item() != label.item():\n",
        "    continue\n",
        "  # Display predicted images \n",
        "  print(\"true label: {} \".format(predicted.detach().cpu().item()))\n",
        "  plt.imshow(np.squeeze(image.detach().cpu()))\n",
        "  plt.show()\n",
        "  # generate adversarial images\n",
        "  image_pert = arbitrary_adversary(model=model, image=image, original_label=label)\n",
        "  # Display images\n",
        "  image_pert = np.squeeze(image_pert)\n",
        "  plt.imshow(image_pert)\n",
        "  plt.show()\n",
        "  cnt += 1\n",
        "  if cnt == 2:\n",
        "    break\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true label: 3 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Attack Iteration: 0, loss: 0.000, current_predict_label: 3\n",
            "\n",
            "\n",
            " Attack Iteration: 10, loss: 2.178, current_predict_label: 8\n",
            "\n",
            "\n",
            " Attack Iteration: 10, misclassified label: 8, Confidence: 0.9561433792114258 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true label: 8 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Attack Iteration: 0, loss: 0.000, current_predict_label: 8\n",
            "\n",
            "\n",
            " Attack Iteration: 10, loss: 0.052, current_predict_label: 8\n",
            "\n",
            "\n",
            " Attack Iteration: 15, misclassified label: 4, Confidence: 0.9158722758293152 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first image is labeled 0 and the network originally labeled it correctly. After we purturbated the image, the network now labeled it as 6. Similarly, the second is 6 originally, and labeled 8 after perturbation. However, if we observe the images before and after the perturbation, we can see that the \"main\" part of the image (the digit part) does not change much intuitively. While through the fast gradient method, we do find changes in the background. Though for human beings, we still focus on the digit itself, and do not get distracted, the perturbated images already fool the network."
      ],
      "metadata": {
        "id": "f0M_WoxNUOwl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgJIt0UjFTNr"
      },
      "source": [
        "2. (10%) For a test image from the dataset, choose a target label yt that you want the network to classify your image as and compute a perturbed image. Note that this is different from what you are asked in part 1, because you want your network to believe that the image has a particular label, not just misclassify the image. You need to modify appropriately the loss function and then perform gradient descent as before. You should still use the constraints from part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "NnHjmnq1FTNr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dce6716-314d-4046-9168-8b562a85ff1d"
      },
      "source": [
        "# don't change the signature of this function (image, image_pert -> [N, 1, H, W])\n",
        "def targeted_adversary(model, image, target_label):\n",
        "  # Setups and Initializations for training\n",
        "  # CUDA for PyTorch\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "  perturbation = torch.zeros(image.size()).to(device)\n",
        "  perturbation.requires_grad = True\n",
        "  max_step = 100000\n",
        "  eps = 0.01\n",
        "  for i in range(max_step):\n",
        "    # Start FGSM Attack, Follow the steps stated above\n",
        "    # perturbation : I_eps in HW document, image_per: I_pert in HW document\n",
        "    model.zero_grad()\n",
        "    # Contraints: take the valid values\n",
        "    output = model((image + perturbation).clamp(min=-1, max=1))\n",
        "    # We want the network to believe that the image has that label, so the loss\n",
        "    # should be -L_CE so that they would be similar (or change the gradient ascent\n",
        "    # to gradient descent: perturbation.data -= eps * sign_perturbation_grad, they are equivalent)\n",
        "    loss = -F.cross_entropy(output, target_label)\n",
        "    loss.backward()\n",
        "    perturbation_grad = perturbation.grad.data\n",
        "    sign_perturbation_grad = perturbation_grad.sign()\n",
        "    # Contraints: perturbation should be positive (do not erase parts of the image)\n",
        "    # perturbation is a leaf tensor, so use .data to take the inplace operation\n",
        "    perturbation.data += eps * sign_perturbation_grad   \n",
        "    perturbation.data = perturbation.clamp(min=0)\n",
        "    # Contraints: take the valid values\n",
        "    image_pert = image + perturbation.data\n",
        "    image_pert = torch.clamp(image_pert, -1, 1)\n",
        "\n",
        "    # Predict and calculate the loss\n",
        "    output_pert = model(image_pert)\n",
        "    predicted = torch.max(output_pert.data, 1)[1]\n",
        "    confidence = torch.max(F.softmax(output_pert.data,dim=1))\n",
        "    if i % 1000 == 1:\n",
        "      print('\\n Attack Iteration: {}, loss: {:.3f}, current_predict_label: {}\\n'.format(i-1,loss.detach().cpu().item(), \n",
        "      predicted.detach().cpu().item()))\n",
        "\n",
        "    if predicted.item() == target_label.item() and confidence>= 0.9:\n",
        "      print('\\n Attack Iteration: {}, predict label: {},target label:{}, Confidence: {}\\n'.format(i-1,predicted.detach().cpu().item(), target_label.detach().cpu().item(),confidence))\n",
        "      image_pert = image_pert.detach().cpu()\n",
        "      break\n",
        "  return image_pert\n",
        "\n",
        "# Display images\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# load the model\n",
        "model_path = 'model.pth'\n",
        "# Instantiate the network\n",
        "model = DigitClassification().to(device)\n",
        "# load the parameters saved\n",
        "model.eval().load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "# choose the label to be 1\n",
        "target_label=torch.tensor([1])\n",
        "cnt=0\n",
        "\n",
        "for i in range(100):\n",
        "  image_iterator=iter(image_loader)\n",
        "  image, label = image_iterator.next()\n",
        "  image, label = image.to(device), label.to(device)\n",
        "  output = model(image)\n",
        "  predicted = torch.max(output.data, 1)[1]\n",
        "  # If the initial prediction is wrong or just the targeted label , dont bother attacking, just move on\n",
        "  if predicted.item() != label.item() or predicted.item() == target_label.item():\n",
        "    continue\n",
        "  # Display predicted images \n",
        "  print(\"true label: {} \".format(predicted.detach().cpu().item()))\n",
        "  plt.imshow(np.squeeze(image.detach().cpu()))\n",
        "  plt.show()\n",
        "  # generate adversarial images\n",
        "  image_pert = targeted_adversary(model=model, image=image, target_label=target_label)\n",
        "  # Display images\n",
        "  image_pert = np.squeeze(image_pert)\n",
        "  plt.imshow(image_pert)\n",
        "  plt.show()\n",
        "  cnt += 1\n",
        "  if cnt == 2:\n",
        "    break\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true label: 8 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Attack Iteration: 0, loss: -19.653, current_predict_label: 8\n",
            "\n",
            "\n",
            " Attack Iteration: 104, predict label: 1,target label:1, Confidence: 0.9015573263168335\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true label: 6 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Attack Iteration: 0, loss: -20.415, current_predict_label: 6\n",
            "\n",
            "\n",
            " Attack Iteration: 83, predict label: 1,target label:1, Confidence: 0.9039269089698792\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASo0lEQVR4nO3dfYxV9ZkH8O933kCG1xEdRxgKKu1KTQUd8aWstXVr0daA3dRK3IYadNqoSd3VtMaa1E3cje2umCatNiNisbo2pvUFE7fVoqnraxkUAUWFyssAI6i8DQPz/uwfczWDzHnOeM6991zm9/0kZO7c555znrnMd86993fO+dHMICLDX1nWDYhIcSjsIoFQ2EUCobCLBEJhFwlERTE3Vj6q2irH1xRk3VWt7QVZbz7wmJH+A/r63LJ1duWxm8P11lS79Yo2f9vW3Z3Pdg43KuZ5O9jh16uPiSx1jR2e+7nuvbvRe7Cdg9VShZ3kXAC/BFAOYImZ3eE9vnJ8DaZe/W9pNhmp/vaXCrLefCj7wqlune3+L23vxk35bOcw++ee49Zrnt/q1nu278hnO4fhF09z69a8zl/Bl74UWWr5p9FJWip5m5csjqwl/vNGshzArwFcDGAGgAUkZyRdn4gUVprXMrMBbDSz98ysC8DvAczLT1sikm9pwj4JQMuA77fl7jsMyUaSzSSbew+W7vtqkeGu4J9SmFmTmTWYWUP5KP/DIBEpnDRh3w6gfsD3k3P3iUgJShP2lQCmk5xGsgrAFQCW56ctEcm3xENvZtZD8noAf0b/0NtSM3szb50NI31vrC/o+ltuPS/xspVtfn1C24HE6waAfVdGD+3tn5byXeTc5D93iFKNs5vZUwCeylMvIlJAw/MwIhE5gsIuEgiFXSQQCrtIIBR2kUAo7CKBKOr57FWt7e6pqGnGiytOmurWN3/3RLdetc9f//F3Z3cKbZrnJU73GL/eMXu6W3/2gftitrA6sjLj7mtjlg1TmtO1d1j0+Sfas4sEQmEXCYTCLhIIhV0kEAq7SCAUdpFAFHXoLU7FIb/eE31lYOxtOMFdtjLmilgj9vuXc+752pnR2z6lyl12YtPLbr2QQ2txLOY34Id3/yHV+s9fe1lkLW6IKe3zQue/1ALczQX4I4uESWEXCYTCLhIIhV0kEAq7SCAUdpFAKOwigSipcfbxG3vc+qGa8sja6EdecZetuHS2W6/a409NXPZC9Kma9vlz3WV3/Di7cfSeanPr7y68J9X69/X5B0cc843oGWgPXO7PIJtWiGPpHj0dIoFQ2EUCobCLBEJhFwmEwi4SCIVdJBAKu0ggSmqcvW2y345FD7PH2jPdX3dVm7/y4985LrLWMZGJeiqGZ6/8r5hHjE61/vN+faNbn1z2amRt30na1wyGFTE56PGPR4mSKuwkNwNoA9ALoMfMGtKsT0QKJx979q+a2Yd5WI+IFJBeR4kEIm3YDcDTJFeRbBzsASQbSTaTbO5GZ8rNiUhSaV/GzzGz7SSPB/AMybfN7PmBDzCzJgBNADCWNf5ZGSJSMKn27Ga2Pfd1F4DHAPinlolIZhKHnWQ1yTEf3wZwEYB1+WpMRPIrzcv4WgCPkfx4Pf9jZn9K00znhDRL+6r2++8gjl3iX9u916360xoXWkdddHeTK9KNo3/xV/60yiM/8p/XllvOTrX9ECUdR4+TOOxm9h6A0/PYi4gUkIbeRAKhsIsEQmEXCYTCLhIIhV0kECV1imsah+b7x/OM3ONPyXzw2/4Q0b5pKc6vLbCvn7WmYOuuv3OVW++ec5pbP3T8iHy2E4SK+sluvadlW6L1as8uEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRi2Iyz75/i/yhjtvknqZZ1+6dqjtsUvfxHM7Idg589Jnpa5LS2/ORMt07/8AVJYOuCKW79xF9onF1EHAq7SCAUdpFAKOwigVDYRQKhsIsEQmEXCURRx9lZVYmKE/xzdZPqGuvXx7zoj0V/8M2T3Xr7idlNy3zut/zz1ReNez/xumetvMKtaxy9+Hqr/HrPhdHHPtir0ZdE155dJBAKu0ggFHaRQCjsIoFQ2EUCobCLBEJhFwlEUcfZras75prX/nm8aWy+5pSCrbvQbjrh6ZhHjEq87s6VNW598rPtbp0vvZF42y23npd42eGsLGbG5t4qZx/tHA4Su2cnuZTkLpLrBtxXQ/IZkhtyXws4s7qI5MNQXsb/FsDcT913M4AVZjYdwIrc9yJSwmLDbmbPA9j9qbvnAViWu70MwPw89yUieZb0PXutmbXmbr8PoDbqgSQbATQCwMgU7y1FJJ3Un8abmQGIvFqjmTWZWYOZNVRCk/yJZCVp2HeSrAOA3Ndd+WtJRAohadiXA1iYu70QwBP5aUdECiX2PTvJhwFcAGAiyW0AfgbgDgCPkFwEYAuAy4eysa66arRcnc3Y6ueWf/ozxsP1rXm7SJ0Msu1/nOXWT702+WcdZzR/163X3/5S4nUPCbO7DsDRqi8mlSM/OBRZY0/0BQhiw25mCyJKF8YtKyKlQ4fLigRCYRcJhMIuEgiFXSQQCrtIIIp6imtFJzB+Y/TQwLgn17rLb/nX0xNv20ZUJl620LbOHVmwdddeucOtF/pK0S0/PTeyZjG7mlAvY13ztj+9uDWvc4odkSXt2UUCobCLBEJhFwmEwi4SCIVdJBAKu0ggFHaRQBR1nL2sow9j/x59aWI7dVridcedqhl5KZ0SUHdWa/yDHLNf/05kbULbhlTrjrPhV2e79dqTd0bW5tS+5y77w2P/z61/+/Vr3Hp389F50ePd/1Du1qsTrld7dpFAKOwigVDYRQKhsIsEQmEXCYTCLhIIhV0kEEUdZ8fBQ7CV/jnrrrmlOcVv3NTDE+a879ZfPO3RVNvf23ZMZG3cV/zLVHeP8n8F7v/NXW59R+9rbv2cVJMA+ZfQfrlhmVv/3xkTI2u3Pvgv7rKlfC699/vWteSVyJr27CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQCrtIIIo7zh6o/5j+eKrlF+8+ya1Pur8qstYTc0n6ypv8YwCmVPhj3VNifoMW75keWXvxo1PcZU8b51/z/t+Pe8Otz6/eG1lbunSLu2zfnuhlAQC9/rXdt9x0hr98BmL37CSXktxFct2A+24juZ3k6ty/SwrbpoikNZSX8b8FMHeQ++8ys5m5f0/lty0RybfYsJvZ8wB2F6EXESmgNB/QXU9yTe5lfuTFvkg2kmwm2dyNzhSbE5E0kob9HgAnA5gJoBXAnVEPNLMmM2sws4ZKpDorQkRSSBR2M9tpZr1m1gfgXgCz89uWiORborCTrBvw7WUAnDlkRaQUxI6zk3wYwAUAJpLcBuBnAC4gORP9l2PfDOAHBezxE3HXhs9KbF9X+eVe80+evvtPF7n1ikXR1+K/Y9Zj7rKXjtrv1g9Zl1sfXeYP5N/7ZHTvVXvpLvvWzBPcetw4+7376iNrtr/NXTZO7xlfSLV8FmLDbmYLBrn7vgL0IiIFpMNlRQKhsIsEQmEXCYTCLhIIhV0kEDrFtQhuv/r7bv3PDy5x6z/+5nK33hhzKqjn8faxbv3nG7/h1suXRF+uGQCmv7o1ukh/6G3PlsluHef75Z+/fHFkrfra6NOCAaBqnz/Jd8exfu+lSHt2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEgiFXSQQGmcvghFv++Pgrb0H3fqisdvceq8/JOz62wH/MtXtf6l165PX7XLrfRPHRdY+PCO6BgDzbnjOrb/Y6e+rRmyvjO4r5jf/aBxHj6M9u0ggFHaRQCjsIoFQ2EUCobCLBEJhFwmEwi4SiKNqnL3vK7Mia2V/fb2InXw2mxb5Y9ljysqL1MmR/rN2jV+/0a/jxuTbjruEdpxTf3edWx/pXKq6x5+JeljSnl0kEAq7SCAUdpFAKOwigVDYRQKhsIsEQmEXCURJjbO33Hpe4mXr/5rHRorsnHv9werbr3zQrc+v3pt423Fj3eUs3P4gbt2Pt4/2lz/on3Me4li6J/Z/kmQ9yedIvkXyTZI/yt1fQ/IZkhtyXycUvl0RSWoof7Z7ANxoZjMAnAPgOpIzANwMYIWZTQewIve9iJSo2LCbWauZvZa73QZgPYBJAOYBWJZ72DIA8wvVpIik95nes5OcCmAWgFcB1JpZa670PoBBL1ZGshFAIwCMhN5EiWRlyJ++kBwN4I8AbjCz/QNrZmYABr3soZk1mVmDmTVUYkSqZkUkuSGFnWQl+oP+kJk9mrt7J8m6XL0OgH+ZURHJVOzLeJIEcB+A9Wa2eEBpOYCFAO7IfX0ibTOMOePRnD9NaYbthmLa/Zsjaz3bk0+ZDADs9es3rbjCrf/zvKZU2/ekPQ31qq0XRNZGlPk/+ItPnu7Wh9/FngtrKO/ZvwzgewDWklydu+8W9If8EZKLAGwBcHlhWhSRfIgNu5m9gOg/ohfmtx0RKRQdLisSCIVdJBAKu0ggFHaRQCjsIoFg/8FvxTGWNXY2k3+Av+2W6LF0bwy+0OpvfynV8oU+RsAz7aHtbr1n05ZU68/yZ/NMeNc/fmD0I6+49VL9uTYvWYyOHS2Djp5pzy4SCIVdJBAKu0ggFHaRQCjsIoFQ2EUCobCLBOKoGmf3lOq4J5B+HL6U7b7qXLfePim7s85r1kefL981xt/PHZh8dJ4tr3F2EVHYRUKhsIsEQmEXCYTCLhIIhV0kEAq7SCCKOmVz3/hROPTV2ZH1thP9dk54YXdkbdqyre6yLd+Z4tYLOb2vnRdz/fOX3ijcxlMq5eMXRu30jxHxxtI7x6UbRz9+Vbdbr17jXydg01VTU20/Ce3ZRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFADGV+9noADwCoBWAAmszslyRvA3ANgA9yD73FzJ5y19ULVO6PPsfYpvjt2DubIms9nZ3usnWLt7n14aqUx8nTqjgUM84+OnosvbzLX7a32x+H33VmpVvHmVP9egojP4ruvawnermhHFTTA+BGM3uN5BgAq0g+k6vdZWb//Rn6FJGMDGV+9lYArbnbbSTXA5hU6MZEJL8+03t2klMBzALwau6u60muIbmU5ISIZRpJNpNs7u5uT9WsiCQ35LCTHA3gjwBuMLP9AO4BcDKAmejf89852HJm1mRmDWbWUFlZnYeWRSSJIYWdZCX6g/6QmT0KAGa208x6zawPwL0Aos9wEZHMxYadJAHcB2C9mS0ecH/dgIddBmBd/tsTkXwZyqfxXwbwPQBrSa7O3XcLgAUkZ6J/OG4zgB/Eraisuxcjdh6IrNc+u8pdvngXvS4tPV87063v+XxVwbZd3uHXy/wzPWHl0TX6syaj5xi/zuhRXABAx8Sj83LQcY675+XI2nsW/bnYUD6NfwHAYM+aO6YuIqVFR9CJBEJhFwmEwi4SCIVdJBAKu0ggFHaRQBT1UtIgYRXRf1/KTj/VXfyDs8ZH1o772x5/01tb/d4m1vj18ugB4953NrqLdlzqH1zYV+mPB4/Y7Q9mj90SfV7j2C3uougZ5f+9rzjkD4ZX7e1y653HjoisHapxBuEB9FX4z8u+k4fnvqpQU3wPz2dLRI6gsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFA0Kx4Z4mT/ADAwJHfiQA+LFoDn02p9laqfQHqLal89vY5MztusEJRw37ExslmM2vIrAFHqfZWqn0B6i2pYvWml/EigVDYRQKRddibMt6+p1R7K9W+APWWVFF6y/Q9u4gUT9Z7dhEpEoVdJBCZhJ3kXJLvkNxI8uYseohCcjPJtSRXk2zOuJelJHeRXDfgvhqSz5DckPs66Bx7GfV2G8ntueduNclLMuqtnuRzJN8i+SbJH+Xuz/S5c/oqyvNW9PfsJMsBvAvg6wC2AVgJYIGZvVXURiKQ3AygwcwyPwCD5PkADgB4wMxOy933CwC7zeyO3B/KCWb2kxLp7TYAB7Kexjs3W1HdwGnGAcwH8H1k+Nw5fV2OIjxvWezZZwPYaGbvmVkXgN8DmJdBHyXPzJ4HsPtTd88DsCx3exn6f1mKLqK3kmBmrWb2Wu52G4CPpxnP9Llz+iqKLMI+CUDLgO+3obTmezcAT5NcRbIx62YGUWtmH19j630AtVk2M4jYabyL6VPTjJfMc5dk+vO09AHdkeaY2RkALgZwXe7lakmy/vdgpTR2OqRpvItlkGnGP5Hlc5d0+vO0sgj7dgD1A76fnLuvJJjZ9tzXXQAeQ+lNRb3z4xl0c193ZdzPJ0ppGu/BphlHCTx3WU5/nkXYVwKYTnIaySoAVwBYnkEfRyBZnfvgBCSrAVyE0puKejmAhbnbCwE8kWEvhymVabyjphlHxs9d5tOfm1nR/wG4BP2fyP8dwE+z6CGir5MAvJH792bWvQF4GP0v67rR/9nGIgDHAlgBYAOAvwCoKaHefgdgLYA16A9WXUa9zUH/S/Q1AFbn/l2S9XPn9FWU502Hy4oEQh/QiQRCYRcJhMIuEgiFXSQQCrtIIBR2kUAo7CKB+H+z73R1EtOXgwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIW1jhNhFTNs"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "3. (10%) Retrain the network from the previous problem. Use some of the adversarial images you generated in parts (1) and (2) and feed them in the retrained network. What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNx3E3qXFTNs"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Sbzhroq4FTNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf056624-3ebc-48b5-af2f-1c2aadd2a451"
      },
      "source": [
        "# adversarial operations and generating new data\n",
        "def arbitrary_adversary(model, image, original_label):\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "  perturbation = torch.zeros(image.size()).to(device)\n",
        "  perturbation.requires_grad = True\n",
        "  max_step = 1000\n",
        "  eps = 0.02\n",
        "  for i in range(max_step):\n",
        "    model.zero_grad()\n",
        "    output = model((image + perturbation).clamp(min=-1, max=1))\n",
        "    loss = F.cross_entropy(output, original_label)\n",
        "    loss.backward()\n",
        "    perturbation_grad = perturbation.grad.data\n",
        "    sign_perturbation_grad = perturbation_grad.sign()\n",
        "    perturbation.data += eps * sign_perturbation_grad   \n",
        "    perturbation.data = perturbation.clamp(min=0)\n",
        "    image_pert = image + perturbation.data\n",
        "    image_pert = torch.clamp(image_pert, -1, 1)\n",
        "    output_pert = model(image_pert)\n",
        "    predicted = torch.max(output_pert.data, 1)[1]\n",
        "    confidence = torch.max(F.softmax(output_pert.data,dim=1))\n",
        "    if predicted.item() != original_label.item() and confidence >= 0.90:\n",
        "      image_pert = image_pert.detach().cpu()\n",
        "      break\n",
        "  return image_pert\n",
        "\n",
        "def targeted_adversary(model, image, target_label):\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "  perturbation = torch.zeros(image.size()).to(device)\n",
        "  perturbation.requires_grad = True\n",
        "  max_step = 100000\n",
        "  eps = 0.01\n",
        "  for i in range(max_step):\n",
        "    model.zero_grad()\n",
        "    output = model((image + perturbation).clamp(min=-1, max=1))\n",
        "    loss = -F.cross_entropy(output, target_label)\n",
        "    loss.backward()\n",
        "    perturbation_grad = perturbation.grad.data\n",
        "    sign_perturbation_grad = perturbation_grad.sign()\n",
        "    perturbation.data += eps * sign_perturbation_grad   \n",
        "    perturbation.data = perturbation.clamp(min=0)\n",
        "    image_pert = image + perturbation.data\n",
        "    image_pert = torch.clamp(image_pert, -1, 1)\n",
        "    output_pert = model(image_pert)\n",
        "    predicted = torch.max(output_pert.data, 1)[1]\n",
        "    confidence = torch.max(F.softmax(output_pert.data,dim=1))\n",
        "    if predicted.item() == target_label.item() and confidence>= 0.9:\n",
        "      image_pert = image_pert.detach().cpu()\n",
        "      break\n",
        "  return image_pert\n",
        "\n",
        "def batch_image_pert(model, image_batch, label_batch, type='arbitrary', target_label=None):\n",
        "  # This function is used to generate batch of perturbated image to train the netwok\n",
        "  # type can be either arbitrary or targeted, if targeted is assigned, target_label should also be assigned\n",
        "  image_pert_batch = torch.zeros_like(image_batch[0][None])\n",
        "  label_pert_batch = torch.zeros_like(label_batch[0][None])\n",
        "  for image, label in zip(image_batch, label_batch):\n",
        "    image = image[None]\n",
        "    label = label[None]\n",
        "    # print(label)\n",
        "    if type == 'arbitrary':\n",
        "      image_pert = arbitrary_adversary(model=model, image=image, original_label=label)\n",
        "    else:\n",
        "      image_pert = targeted_adversary(model, image, target_label=target_label)\n",
        "    label_pert = torch.argmax(model(image_pert), dim=1)\n",
        "    image_pert_batch = torch.cat((image_pert_batch,image_pert), dim=0)\n",
        "    label_pert_batch = torch.cat((label_pert_batch,label_pert), dim=0)\n",
        "  return image_pert_batch[1:, :, :, :], label_pert_batch[1:]\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "image_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform),\n",
        "    batch_size=2000, shuffle=True)\n",
        "\n",
        "# generate 2000 adversirial images for retraining, 1500 would be mixed in orginial minist trainning set, the other 500 would be used for testing\n",
        "# comment for sake of not running timeout on aotuograder, uncomment to see full code\n",
        "'''\n",
        "image_batch, label_batch = next(iter(image_loader))\n",
        "image_pert_batch, label_pert_batch = batch_image_pert(model, image_batch, label_batch)\n",
        "print(label_batch)\n",
        "print(label_pert_batch)\n",
        "# make sure the image is really attacked\n",
        "print(label_batch.eq(label_pert_batch).sum().item()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 8, 9,  ..., 7, 1, 1])\n",
            "tensor([9, 9, 4,  ..., 2, 4, 7])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define training and testing process for adverserial datasets\n",
        "def train_adv(Net, device, train_loader, optimizer, criterion, epoch, display_interval, train_pert, train_label):\n",
        "  Net.train()\n",
        "  running_loss = 0\n",
        "  running_correct = 0\n",
        "  cnt=0\n",
        "  for batch_index, (inputs, labels) in enumerate(train_loader,0):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()   # remember to clear the gradients\n",
        "    outputs = Net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    optimizer.step()\n",
        "    pred_outputs = torch.max(outputs.data, dim=1)[1] # get the prediction\n",
        "    running_correct += (pred_outputs == labels).sum()\n",
        "    \n",
        "    inputs_adv, labels_adv = train_pert[(batch_index)*3:(batch_index+1)*3,:,:,:].to(device), train_label[(batch_index)*3:(batch_index+1)*3].to(device)\n",
        "    outputs = Net(inputs_adv)\n",
        "    loss = criterion(outputs, labels_adv)\n",
        "    loss.backward()\n",
        "    running_loss += loss.item() * inputs_adv.size(0)\n",
        "    optimizer.step()\n",
        "    pred_outputs = torch.max(outputs.data, dim=1)[1]\n",
        "    running_correct += (pred_outputs == labels_adv).sum()\n",
        "\n",
        "    # Print the training process\n",
        "    if batch_index % display_interval == 0:\n",
        "      print('Train Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
        "          epoch+1, batch_index*(len(inputs)+3), len(train_loader.dataset)+1500, 100.*batch_index / len(train_loader),\n",
        "          loss.item(), float(running_correct*100) / float((train_batch_size+3)*(batch_index + 1))))\n",
        "  epoch_loss = running_loss / (len(train_loader.dataset)+1500)\n",
        "  epoch_accu = (running_correct) / (len(train_loader.dataset)+1500)\n",
        "  train_loss.append(epoch_loss)\n",
        "  train_accu.append(epoch_accu)\n",
        "  print('\\nTrain set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        epoch_loss, running_correct, len(train_loader.dataset)+len(train_label),\n",
        "        100. * epoch_accu))\n",
        "\n",
        "def test_adv(Net, device, test_loader, criterion, epoch, test_pert, test_label):\n",
        "  Net.eval()\n",
        "  running_loss = 0\n",
        "  running_correct = 0\n",
        "  with torch.no_grad(): # normal MINST set\n",
        "    for batch_index, (inputs, labels) in enumerate(test_loader,0):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = Net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      pred_outputs = torch.max(outputs.data, dim=1)[1]\n",
        "      running_correct += (pred_outputs == labels).sum()\n",
        "\n",
        "      inputs_adv, labels_adv = test_pert[(batch_index)*5:(batch_index+1)*5,:,:,:].to(device), test_label[(batch_index)*5:(batch_index+1)*5].to(device)\n",
        "      outputs = Net(inputs_adv)\n",
        "      loss = criterion(outputs, labels_adv)\n",
        "      running_loss += loss.item() * inputs_adv.size(0)\n",
        "      pred_outputs = torch.max(outputs.data, dim=1)[1]\n",
        "      running_correct += (pred_outputs == labels_adv).sum()\n",
        "      \n",
        "  epoch_loss =  running_loss / (len(test_loader.dataset)+500)\n",
        "  epoch_accu = (running_correct) / (len(test_loader.dataset)+500)\n",
        "  test_loss.append(epoch_loss)\n",
        "  test_accu.append(epoch_accu)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        epoch_loss, running_correct, len(test_loader.dataset)+len(test_label),\n",
        "        100. * epoch_accu))"
      ],
      "metadata": {
        "id": "zHLKCbLUakbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the generated datasets into train and test sets\n",
        "train_pert, train_label = image_pert_batch[0:1500,:,:,:], label_batch[0:1500]\n",
        "test_pert, test_label = image_pert_batch[1500:2000,:,:,:], label_batch[1500:2000]"
      ],
      "metadata": {
        "id": "YbAEjsoLHzTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain the network (same structure that of Project 1.a)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# load the old weight for comparison\n",
        "model_path = 'model.pth'\n",
        "Net_original = DigitClassification().to(device)\n",
        "Net_original.eval().load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "# New weights that would be trained and compared\n",
        "Net_adv = DigitClassification().to(device)\n",
        "LR = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(Net_original.parameters(),lr=LR)\n",
        "optimizer_adv = torch.optim.Adam(Net_adv.parameters(),lr=LR)\n",
        "display_interval = 100\n",
        "train_loss = []\n",
        "train_accu = []\n",
        "test_loss = []\n",
        "test_accu = []\n",
        "train_batch_size=120\n",
        "test_batch_size=100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform),\n",
        "    batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform),\n",
        "    batch_size=test_batch_size, shuffle=True, num_workers=8)\n",
        "# Net, device, train_loader has been setup before\n",
        "# comment for sake of not running timeout on aotuograder, uncomment to see full code\n",
        "'''\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "  train_adv(Net=Net_adv, device=device, train_loader=train_loader, optimizer=optimizer_adv, criterion=criterion, epoch=epoch, display_interval=display_interval,train_pert=train_pert,train_label=train_label)\n",
        "  test_adv(Net=Net_adv, device=device, test_loader=test_loader, criterion=criterion, epoch=epoch, test_pert=test_pert, test_label=test_label)\n",
        "test_adv(Net=Net_original, device=device, test_loader=test_loader, criterion=criterion, epoch=1, test_pert=test_pert, test_label=test_label)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbC389KmBIOI",
        "outputId": "24519fb5-216b-41dc-e5c9-7caa470f1c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch : 1 [0/61500 (0%)]\tLoss: 2.111973\t Accuracy:18.699%\n",
            "Train Epoch : 1 [12300/61500 (20%)]\tLoss: 0.117307\t Accuracy:81.035%\n",
            "Train Epoch : 1 [24600/61500 (40%)]\tLoss: 1.527567\t Accuracy:86.276%\n",
            "Train Epoch : 1 [36900/61500 (60%)]\tLoss: 0.134211\t Accuracy:87.772%\n",
            "Train Epoch : 1 [49200/61500 (80%)]\tLoss: 0.015776\t Accuracy:88.407%\n",
            "\n",
            "Train set: Avg. loss: 0.3468, Accuracy: 54921/61500 (89%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.2225, Accuracy: 9738/10500 (93%)\n",
            "\n",
            "Train Epoch : 2 [0/61500 (0%)]\tLoss: 0.037879\t Accuracy:95.935%\n",
            "Train Epoch : 2 [12300/61500 (20%)]\tLoss: 0.159039\t Accuracy:92.288%\n",
            "Train Epoch : 2 [24600/61500 (40%)]\tLoss: 0.538116\t Accuracy:92.885%\n",
            "Train Epoch : 2 [36900/61500 (60%)]\tLoss: 0.145938\t Accuracy:93.374%\n",
            "Train Epoch : 2 [49200/61500 (80%)]\tLoss: 0.047084\t Accuracy:93.709%\n",
            "\n",
            "Train set: Avg. loss: 0.1919, Accuracy: 57840/61500 (94%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.1706, Accuracy: 9912/10500 (94%)\n",
            "\n",
            "Train Epoch : 3 [0/61500 (0%)]\tLoss: 0.016802\t Accuracy:96.748%\n",
            "Train Epoch : 3 [12300/61500 (20%)]\tLoss: 0.062427\t Accuracy:94.687%\n",
            "Train Epoch : 3 [24600/61500 (40%)]\tLoss: 0.660312\t Accuracy:95.183%\n",
            "Train Epoch : 3 [36900/61500 (60%)]\tLoss: 0.014894\t Accuracy:95.314%\n",
            "Train Epoch : 3 [49200/61500 (80%)]\tLoss: 0.007417\t Accuracy:95.473%\n",
            "\n",
            "Train set: Avg. loss: 0.1397, Accuracy: 58832/61500 (96%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.1200, Accuracy: 10090/10500 (96%)\n",
            "\n",
            "Train Epoch : 4 [0/61500 (0%)]\tLoss: 0.003359\t Accuracy:95.122%\n",
            "Train Epoch : 4 [12300/61500 (20%)]\tLoss: 0.028041\t Accuracy:96.176%\n",
            "Train Epoch : 4 [24600/61500 (40%)]\tLoss: 0.583249\t Accuracy:96.643%\n",
            "Train Epoch : 4 [36900/61500 (60%)]\tLoss: 0.006786\t Accuracy:96.451%\n",
            "Train Epoch : 4 [49200/61500 (80%)]\tLoss: 0.000381\t Accuracy:96.363%\n",
            "\n",
            "Train set: Avg. loss: 0.1192, Accuracy: 59298/61500 (96%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.1285, Accuracy: 10051/10500 (96%)\n",
            "\n",
            "Train Epoch : 5 [0/61500 (0%)]\tLoss: 0.010847\t Accuracy:95.122%\n",
            "Train Epoch : 5 [12300/61500 (20%)]\tLoss: 0.019004\t Accuracy:96.442%\n",
            "Train Epoch : 5 [24600/61500 (40%)]\tLoss: 0.806139\t Accuracy:96.659%\n",
            "Train Epoch : 5 [36900/61500 (60%)]\tLoss: 0.007993\t Accuracy:96.629%\n",
            "Train Epoch : 5 [49200/61500 (80%)]\tLoss: 0.001817\t Accuracy:96.636%\n",
            "\n",
            "Train set: Avg. loss: 0.1082, Accuracy: 59508/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.1321, Accuracy: 10059/10500 (96%)\n",
            "\n",
            "Train Epoch : 6 [0/61500 (0%)]\tLoss: 0.002663\t Accuracy:97.561%\n",
            "Train Epoch : 6 [12300/61500 (20%)]\tLoss: 0.046240\t Accuracy:96.321%\n",
            "Train Epoch : 6 [24600/61500 (40%)]\tLoss: 0.428337\t Accuracy:96.821%\n",
            "Train Epoch : 6 [36900/61500 (60%)]\tLoss: 0.231183\t Accuracy:96.880%\n",
            "Train Epoch : 6 [49200/61500 (80%)]\tLoss: 0.032955\t Accuracy:96.892%\n",
            "\n",
            "Train set: Avg. loss: 0.1010, Accuracy: 59629/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0662, Accuracy: 10271/10500 (98%)\n",
            "\n",
            "Train Epoch : 7 [0/61500 (0%)]\tLoss: 0.005374\t Accuracy:100.000%\n",
            "Train Epoch : 7 [12300/61500 (20%)]\tLoss: 0.002795\t Accuracy:96.989%\n",
            "Train Epoch : 7 [24600/61500 (40%)]\tLoss: 0.401394\t Accuracy:97.116%\n",
            "Train Epoch : 7 [36900/61500 (60%)]\tLoss: 0.004921\t Accuracy:97.086%\n",
            "Train Epoch : 7 [49200/61500 (80%)]\tLoss: 0.001681\t Accuracy:97.180%\n",
            "\n",
            "Train set: Avg. loss: 0.0880, Accuracy: 59855/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0832, Accuracy: 10260/10500 (98%)\n",
            "\n",
            "Train Epoch : 8 [0/61500 (0%)]\tLoss: 0.018280\t Accuracy:95.935%\n",
            "Train Epoch : 8 [12300/61500 (20%)]\tLoss: 0.003095\t Accuracy:96.965%\n",
            "Train Epoch : 8 [24600/61500 (40%)]\tLoss: 0.147481\t Accuracy:97.185%\n",
            "Train Epoch : 8 [36900/61500 (60%)]\tLoss: 0.000649\t Accuracy:97.215%\n",
            "Train Epoch : 8 [49200/61500 (80%)]\tLoss: 0.003276\t Accuracy:97.168%\n",
            "\n",
            "Train set: Avg. loss: 0.0899, Accuracy: 59811/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0591, Accuracy: 10297/10500 (98%)\n",
            "\n",
            "Train Epoch : 9 [0/61500 (0%)]\tLoss: 0.005788\t Accuracy:96.748%\n",
            "Train Epoch : 9 [12300/61500 (20%)]\tLoss: 0.178693\t Accuracy:95.935%\n",
            "Train Epoch : 9 [24600/61500 (40%)]\tLoss: 0.094022\t Accuracy:96.780%\n",
            "Train Epoch : 9 [36900/61500 (60%)]\tLoss: 0.000843\t Accuracy:96.959%\n",
            "Train Epoch : 9 [49200/61500 (80%)]\tLoss: 0.002562\t Accuracy:97.046%\n",
            "\n",
            "Train set: Avg. loss: 0.0952, Accuracy: 59765/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0812, Accuracy: 10217/10500 (97%)\n",
            "\n",
            "Train Epoch : 10 [0/61500 (0%)]\tLoss: 0.001522\t Accuracy:98.374%\n",
            "Train Epoch : 10 [12300/61500 (20%)]\tLoss: 0.002417\t Accuracy:97.440%\n",
            "Train Epoch : 10 [24600/61500 (40%)]\tLoss: 0.343450\t Accuracy:97.634%\n",
            "Train Epoch : 10 [36900/61500 (60%)]\tLoss: 0.000194\t Accuracy:97.647%\n",
            "Train Epoch : 10 [49200/61500 (80%)]\tLoss: 0.000860\t Accuracy:97.417%\n",
            "\n",
            "Train set: Avg. loss: 0.0845, Accuracy: 59911/61500 (97%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0574, Accuracy: 10299/10500 (98%)\n",
            "\n",
            "Train Epoch : 11 [0/61500 (0%)]\tLoss: 0.018682\t Accuracy:99.187%\n",
            "Train Epoch : 11 [12300/61500 (20%)]\tLoss: 0.001341\t Accuracy:97.698%\n",
            "Train Epoch : 11 [24600/61500 (40%)]\tLoss: 0.082571\t Accuracy:97.779%\n",
            "Train Epoch : 11 [36900/61500 (60%)]\tLoss: 0.009424\t Accuracy:97.720%\n",
            "Train Epoch : 11 [49200/61500 (80%)]\tLoss: 0.000453\t Accuracy:97.869%\n",
            "\n",
            "Train set: Avg. loss: 0.0688, Accuracy: 60236/61500 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0687, Accuracy: 10255/10500 (98%)\n",
            "\n",
            "Train Epoch : 12 [0/61500 (0%)]\tLoss: 0.001017\t Accuracy:98.374%\n",
            "Train Epoch : 12 [12300/61500 (20%)]\tLoss: 0.000083\t Accuracy:98.036%\n",
            "Train Epoch : 12 [24600/61500 (40%)]\tLoss: 0.217160\t Accuracy:98.091%\n",
            "Train Epoch : 12 [36900/61500 (60%)]\tLoss: 0.001215\t Accuracy:98.061%\n",
            "Train Epoch : 12 [49200/61500 (80%)]\tLoss: 0.002518\t Accuracy:98.090%\n",
            "\n",
            "Train set: Avg. loss: 0.0681, Accuracy: 60273/61500 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0603, Accuracy: 10313/10500 (98%)\n",
            "\n",
            "Train Epoch : 13 [0/61500 (0%)]\tLoss: 0.000331\t Accuracy:100.000%\n",
            "Train Epoch : 13 [12300/61500 (20%)]\tLoss: 0.000042\t Accuracy:97.907%\n",
            "Train Epoch : 13 [24600/61500 (40%)]\tLoss: 0.011920\t Accuracy:97.982%\n",
            "Train Epoch : 13 [36900/61500 (60%)]\tLoss: 0.000265\t Accuracy:97.577%\n",
            "Train Epoch : 13 [49200/61500 (80%)]\tLoss: 0.000680\t Accuracy:97.502%\n",
            "\n",
            "Train set: Avg. loss: 0.0806, Accuracy: 60019/61500 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0673, Accuracy: 10279/10500 (98%)\n",
            "\n",
            "Train Epoch : 14 [0/61500 (0%)]\tLoss: 0.000928\t Accuracy:99.187%\n",
            "Train Epoch : 14 [12300/61500 (20%)]\tLoss: 0.000229\t Accuracy:98.221%\n",
            "Train Epoch : 14 [24600/61500 (40%)]\tLoss: 0.054832\t Accuracy:98.249%\n",
            "Train Epoch : 14 [36900/61500 (60%)]\tLoss: 0.000019\t Accuracy:98.231%\n",
            "Train Epoch : 14 [49200/61500 (80%)]\tLoss: 0.008351\t Accuracy:98.125%\n",
            "\n",
            "Train set: Avg. loss: 0.0661, Accuracy: 60318/61500 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0460, Accuracy: 10349/10500 (99%)\n",
            "\n",
            "Train Epoch : 15 [0/61500 (0%)]\tLoss: 0.000006\t Accuracy:97.561%\n",
            "Train Epoch : 15 [12300/61500 (20%)]\tLoss: 0.000017\t Accuracy:98.487%\n",
            "Train Epoch : 15 [24600/61500 (40%)]\tLoss: 0.321298\t Accuracy:98.398%\n",
            "Train Epoch : 15 [36900/61500 (60%)]\tLoss: 0.000025\t Accuracy:98.169%\n",
            "Train Epoch : 15 [49200/61500 (80%)]\tLoss: 0.000242\t Accuracy:98.202%\n",
            "\n",
            "Train set: Avg. loss: 0.0631, Accuracy: 60404/61500 (98%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0430, Accuracy: 10359/10500 (99%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.2272, Accuracy: 9936/10500 (95%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retrain and comparison setups are described below:\n",
        "1. **Dataset**: MNIST dataset is a large dataset. Therefore, 2000 adversarial images are randomly picked and generated from the MNIST dataset using the `arbitrary_adversary` function defined previously to provide sufficient attacks. The generated images are split into 1500 training images and 500 testing images. \n",
        "2. **Retraining setup:** For the retrained network, it has the same structure as that of Project 1.a, but the weights are trained from the new 61500 (60000 original+1500 adversarial) images. For sake of a fair comparison, the hyperparameters (except for the epochs) are set to be the same as Project 1.a\n",
        "2. **Test and Comparison:** To show the effect of thr adversary and how networks adapt to the attack. The original network (which is trained on 60000 MINIST training dataset and gets over 99% accuracy on 10000 MINIST testing dataset in Project 1.a) is loaded. The retrained and original networks would run the same 10500(10000 original+500 adversarial) test images and their performances would be compared.\n",
        "\n",
        "\n",
        "Observations:\n",
        "The processes and outcomes are shown in the above blocks\n",
        "1. First of all, with the same hyperparameter and network structure, the retrained network is much harder to train than the original network.  In project 1.a, it takes 10 epochs for the network to achieve 99% accuracy both on the training and testing set. In the retrained process, it would take 15 epochs for the network to first reach 98% accuracy on the training set and 99% accuracy on the testing set. This probably implies the effective attack caused by adversarial images.\n",
        "2. When the original network test the new testing set, it only gets 95% accuracy. Considering that the adversarial images take about 5% portion of the testing set, this probably means that the original network misclassifies most of the adversarial images. While the retrained network can achieve 99% accuracy. This phenomenon is understadable, since the gradient used to generate the adversarial images is derived from the original model, the attack itself is targeted at the model. So, when the weights or network structure changed, the influence faded.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QVbzBFSkEfnH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5KpnmwiFTNu"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## References\n",
        "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/main/HW1/images/refs.png\"/, width=600\n",
        "         ></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S1HDcTOFTNu"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Make sure you have run all cells in your notebook in order before you zip together your submission, so that all images/graphs appear in the output. \n",
        "\n",
        "For part (b), your submission should consist of two files: this notebook and the saved weights from question 3. There is no need to upload the new, retrained, weights.\n",
        "\n",
        "**Please save before exporting!**"
      ]
    }
  ]
}